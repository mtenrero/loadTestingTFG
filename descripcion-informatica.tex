\cleardoublepage
\chapter{Descripción informática}


\section{Requisitos}
\label{sec:requisitos}

Tras un análisis del proyecto a desarrollar se llegó a la conclusión de que había que seguir un enfoque top-down, o de arriba hacia abajo. Era necesario que durante toda la etapa del desarrollo, se tuviese muy claro a dónde se pretendía llegar. Siendo los objetivos principales los siguientes:

\begin{itemize}
  \item Capacidad de desplegar los servicios Docker necesarios para una tarea específica
  \item Networking automático y transparente
  \item Interfaz sencilla para el usuario
  \item Integración completa con Apache JMeter
\end{itemize}

Así se mostró en el tablero Kanban del proyecto, proporcionado por GitHub para el repositorio concreto\footnote{https://github.com/mtenrero/ATQ-Director}.


\section{Arquitectura y Análisis}
\label{sec:arquitectura-analisis}


Golang fue el lenguaje de programación elegido debido a su alta simplicidad en cuanto a concurrencia se refiere, por permitir ser compilado y generar binarios para los mayores Sistemas Operativos y debido a la existencia de un paquete distribuido por Docker para comunicarse directamente con la API que ofrece el demonio de Docker.

La mayor complejidad presente en este proyecto ha sido obtener las direcciones virtuales de cada contenedor desplegado sobre el cluster Swarm, ya que son necesarias para poder comunicarse con las instancias directamente. Docker, por defecto, cuando crea una red entre servicios, puede invocar directamente al nombre del servicio para acceder a él, pero en el caso de que se despligue el servicio en modo replicado y éste haya sido configurado para tener más de una replica, por defecto, Docker actúa como un balanceador de carga y sólo responde con una única dirección IP correspondiente a un único contenedor. Esto ha sido un duro handicap, ya que se requería conocer todas las direcciones de los contenedores desplegados.\newline


Durante el análisis de cómo afrontar esta problemática, surgieron dos enfoques totalmente diferentes


\subsection{Primer enfoque: Starter en cada imagen a desplegar}

Este enfoque surge tomando como referencia la mayoría de herramientas para service discovery usadas en Docker y en Kubernetes como Consul.io\footnote{https://www.consul.io/}. 

Cada contenedor a desplegar, tiene una imagen modificada, la cual, antes de ejecutar el entrypoint predefinido en la imagen, ejecuta una pequeña aplicación, la cual, obtiene la IP virtual del contenedor y se la comunica al agente controlador, el cual lleva un listado de todas los contenedores descubiertos. 

%% INSERTAR IMAGEN CON LA ARQUITECTURA

Este enfoque, implicaba modificar todas las posibles imágenes a usar con la aplicación, con lo que se reducía drásticamente la facilidad de uso con otras aplicaciones al requerir que en caso de no existir la imagen modificada deseada a desplegar con Automation Test Queue, obligaría al usuario a modificarla por él mismo.

\subsection{Segundo enfoque: Single Daemon y DNS Round Robin}

Con una configuración de red entre servicios Docker configurada para operar con el algoritmo DNS Round Robin, se consigue disponer de la lista de todas las direcciones IP Virtuales de los contenedores de un mismo servicio.\newline

Disponiendo de esta información, dejaría de ser necesario un descubrimiento de servicios como el expuesto en el anterior punto, dotando a la aplicación de la sencillez de uso deseada debido a que el usuario ya no tendría que modificar la imagen a utilizar.

%% INSERTAR IMAGEN CON LA ARQUITECTURA

\section{Diseño e Implementación}
\label{sec:diseno-implementacion}

Aunque, la planificación de la herramienta se ha realizado con un enfoque bottom-up, el diseño de la misma va a ser expuesto utilizando el enfoque contrario, top-down, es decir, desde lo mas abstracto a lo más específico. 

\subsection{API REST}

El diseño de la interfaz con la que trabajará el usuario que utilice esta aplicación ha sido diseñada meticulosamente utilizando el framework de Go \textit{GoaDesign}, el cual, obliga a definir el diseño de la API con un lenguaje declarativo propio antes de la implementación.\newline

Se define el concepto \textbf{Tarea} como la definición de las pruebas a ejecutar usando el framework Automation Test Queue.\newline 

%% INSERTAR ESTRUCTURA DE LA TAREA

Se ofrecen diferentes acciones descritas a continuación

\begin{itemize}
  \item \textbf{/databind} Operaciones relacionadas con la gestión de archivos
  \subitem \textit{GET /list} Devuelve una lista de los ficheros disponibles en el orquestador
  \subitem \textit{POST /upload} Subir un fichero \*.zip con contenidos para que esté disponible para una futura tarea
  \item \textbf{/monitoring} Monitorización del framework
  \subitem \textit{/ping} Devuelve un HTTP 200OK si el orquestador está operativo
  \item \textbf{/swarm} Estado del cluster Swarm
  \subitem \textit{GET /} Devuelve los detalles del Cluster Swarm
  \item \textbf{/task} Operaciones con las tareas a lanzar con el framework
      \subitem \textit{PUT /task} Crea una nueva tarea
      \subitem \textit{DELETE /task/\{id\}} Elimina una tarea ya planificada
      \subitem \textit{GET /task/\{id\}} Inspecciona una tarea planificada
\end{itemize}

\subsection{Fase de orquestación}
\subsection{Imágenes Docker}
\subsection{Service Discovery}

\section{Integración Continua}
\label{sec:integracion-continua}

Inicialmente, el proyecto, al estar alojado en el servicio de control de versiones de GitHub, en un repositorio de código abierto, se ofrecían diversas opciones SAAS de Integración Continua, entre ellas TravisCI o CircleCI.


Al comenzar el desarrollo, las características que ofrecía TravisCI encajaban perfectamente con los requisitos de Automation Test Queue:

\begin{itemize}
  \item Pipeline-as-code
  \item Enfocado a Contenedores
  \item Integración con GitHub
\end{itemize}

Se utilizó durante la mitad de desarrollo del proyecto y ofreció muy buenos resultados. Pero llegó un momento en el que era necesario que el host donde se ejecutaban los tests cada vez que realizaba un commit formase parte de un cluster de Docker Swarm, y esto no se podía conseguir con la solución que ofrecía TravisCI.\newline

La limitación de TravisCI era muy importante ya que rompía la línea de Integración Continua, por lo que no quedó mas remedio que optar por una solución mas configurable, Jenkins.

No se optó desde un principio por Jenkins debido a la necesidad de disponer de una máquina con acceso a la red las 24 horas al día y la configuración del mismo.\newline

El despliegue de las máquinas destinadas a la línea de integración continua se realizó sobre máquinas EC2 de Amazon Web Services, todas ellas bajo el mismo grupo de disponibilidad y con un servicio de IP elástica, permitiendo de esta manera que la dirección de acceso a Jenkins fuese estática. Además se incluyó un registro A en el DNS de mi dominio particular para poder acceder con una dirección\footnote{http://atq.mtenrero.com:8080} mas fácil de recordar aún.

Una parte vital para el correcto funcionamiento de la línea de integración continua fue configurar webhooks en GitHub para apuntar a la instancia de Jenkins además de habilitar la integración propia de Jenkins. Esto permitió que con cada evento en el repositorio, como \textit{commits} o \textit{pull-requests} se enviase una notificación a Jenkins para así poder ejecutar el job especificado y lanzar los tests unitarios sobre todas las ramas del proyecto para asegurar la regresión y un buen funcionamiento del mismo.\newline


Hace apenas dos años, Cloudbees, la organización que es oficialmente responsable del desarrollo de Jenkins, lanzó una característica que llamaron \textit{Jenkins Declarative Pipelines} \footnote{https://jenkins.io/blog/2017/02/03/declarative-pipeline-ga/}. Estos pipelines, permiten escribir, de una forma desriptiva, las tareas y procesos a realizar cada vez que el job es ejecutado.

Esto ha permitido describir el workflow de ejecución de manera totalmente agnóstica y compatible con cualquier Jenkins desplegado, siempre que tenga Docker instalado, ya que hace uso del denominado Docker-in-Docker, que permite utilizar el demonio de Docker del host dentro de un contenedor ya existente.


\section{Pruebas}
\label{sec:pruebas}

Hablar de Cobertura de Test, como se ha organizado por paquetes, cómo está integrado con el Pipeline de IC de Jenkins, los triggers que tiene configurado...
\newline

Explicar cómo se diseñaron primero los tests básicos, luego la implementación, y después se amplió la cobertura de tests
