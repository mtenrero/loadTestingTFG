\cleardoublepage
\chapter{Descripción informática}


\section{Requisitos}
\label{sec:requisitos}

Teniendo en cuenta las carencias observadas durante el despliegue manual y tradicional de Apache JMeter se observaron diferentes puntos a mejorar:

\begin{itemize}
  \item Configuración manual nodo a nodo
  \item Obligatoriedad de administración y mantenimiento de la infraestructura
  \item Seguridad en las comunicaciones de red
  \item Crecimiento horizontal real
\end{itemize}

Tras un análisis del proyecto a desarrollar, se llegó a la conclusión de que había que seguir un enfoque top-down, o de arriba hacia abajo. Era necesario que durante toda la etapa del desarrollo, se tuviese muy claro a dónde se pretendía llegar. Siendo los objetivos principales los siguientes:

\begin{itemize}
  \item \textbf{Capacidad de desplegar los servicios Docker necesarios para una tarea específica} de manera transparente y automática
  \item \textbf{Networking automático y transparente} que cifre o aísle las comunicaciones entre todos los contenedores.
  \item \textbf{Interfaz sencilla para el usuario} para crear una capa de abstracción de la administración de sistemas tradicional.
  \item \textbf{Integración completa con Apache JMeter} ya que es la herramienta en la que se basa el proyecto, sin olvidar que el middleware o framework a desarrollar debería permitir ser usado con cualquier otra herramienta.
\end{itemize}

Así se mostró en el tablero Kanban del proyecto, proporcionado por GitHub para el repositorio concreto\footnote{https://github.com/mtenrero/ATQ-Director}.


\section{Arquitectura y Análisis}
\label{sec:arquitectura-analisis}


Golang fue el lenguaje de programación elegido debido a su alta simplicidad en cuanto a concurrencia se refiere, por permitir ser compilado y generar binarios para los mayores Sistemas Operativos y debido a la existencia de un paquete distribuido por Docker para comunicarse directamente con la API que ofrece el demonio de Docker.

La mayor complejidad presente en este proyecto ha sido obtener las direcciones virtuales de cada contenedor desplegado sobre el cluster Swarm, ya que son necesarias para poder comunicarse con las instancias directamente. Docker, por defecto, cuando crea una red entre servicios, puede invocar directamente al nombre del servicio para acceder a él, pero en el caso de que se despligue el servicio en modo replicado y éste haya sido configurado para tener más de una replica, por defecto, Docker actúa como un balanceador de carga y sólo responde con una única dirección IP correspondiente a un único contenedor. Esto ha sido un duro handicap, ya que se requería conocer todas las direcciones de los contenedores desplegados.\newline


Durante el análisis de cómo afrontar esta problemática, surgieron dos enfoques totalmente diferentes


\subsection{Primer enfoque: Starter en cada contenedor a desplegar}

Este enfoque\footnote{https://github.com/mtenrero/AutomationTestQueue} surge tomando como referencia la mayoría de herramientas para service discovery usadas en Docker y en Kubernetes como Consul.io\footnote{https://www.consul.io/}.

Cada contenedor a desplegar, tiene una imagen modificada, la cual, antes de ejecutar el entrypoint predefinido en la imagen, ejecuta una pequeña aplicación, la cual, obtiene la IP virtual del contenedor y se la comunica al agente controlador, el cual lleva un listado de todas los contenedores descubiertos.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{old_arch}
    \caption{Arquitectura 0 Starter en cada contenedor}
    \label{fig:old_arch}
\end{figure}

Este enfoque, implicaba modificar todas las posibles imágenes a usar con la aplicación, con lo que se reducía drásticamente la facilidad de uso con otras aplicaciones al requerir que en caso de no existir la imagen modificada deseada a desplegar con Automation Test Queue, obligaría al usuario a modificarla por él mismo.

\subsection{Segundo enfoque: Single Daemon y DNS Round Robin}

Con una configuración de red entre servicios Docker configurada para operar con el algoritmo DNS Round Robin, se consigue disponer de la lista de todas las direcciones IP Virtuales de los contenedores de un mismo servicio.\footnote{https://github.com/mtenrero/ATQ-Director}\newline

Disponiendo de esta información, dejaría de ser necesario un descubrimiento de servicios como el expuesto en el anterior punto, dotando a la aplicación de la sencillez de uso deseada debido a que el usuario ya no tendría que modificar la imagen a utilizar.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{new_arch}
    \caption{Arquitectura 1 DNS RR con Auto Discovery}
    \label{fig:new_arch}
\end{figure}

De esta manera, el middleware \textit{\textbf{Automation Test Queue}} se comunicaría directamente con el demonio de Docker a través del SDK proporcionado por la comunidad y se encargaría de toda la orquestación necesaria para el despliegue automático de test de carga en el clúster.\newline

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{flyview_arch}
    \caption{ATQ low level architecture}
    \label{fig:flyview_arch}
\end{figure}


\subsection{Estructura del proyecto}

El proyecto está íntegramente codificado en \textit{Go}. Sigue una estructura típica de este lenguaje, siempre teniendo en cuenta que se mencionan rutas relativas a la ubicación del repositorio (/github.com/mtenrero/ATQ-Director) bajo el \$GOPATH definido en el sistema.\newline

\dirtree{%
.1 \textbf{app} Código autogenerado por \textit{Goa}.
.1 \textbf{client} Implementación de un cliente en Go, autogenerado.
.1 \textbf{configLoader} Implementación de la configuración externalizada.
.1 \textbf{dnsdiscovery} Implementación discovery DNS.
.1 \textbf{dockerMiddleware} Middleware Docker.
.1 \textbf{http}.
.2 \textbf{design} Diseño API REST con el DSL de \textit{Goa}.
.1 \textbf{persistance} Implementación persistencia datos.
.1 \textbf{swagger} Especificación OpenAPI autogenerada.
.1 \textbf{tool} Helper autogenerado para trabajar con la API.
.1 \textbf{types} Mapeado tipos Goa/Middleware.
.1 \textit{monitoring.go} Controlador Monitoring.
.1 \textit{swarm.go} Controlador Swarm.
.1 \textit{task.go} Controlador Task.
.1 \textit{databind.go} Controlador Databind.
}

En el esquema anteriormente presentado, se puede apreciar que la implementación ha sido fragmentada en grandes paquetes para cumplir el estándar de \textit{Go} y de los principios \textit{KISS} y \textit{SOLID}.\newline

En la raíz del directorio del proyecto se pueden apreciar unos ficheros con extensión \textit{.go}, contienen los controladores para cada endpoint definido con el DSL de \textit{Goa} en el paquete \textit{http/design}. Se genera la estructura básica en la primera ejecución de \textit{goagen} y a partir de ahí se integra la implementación propia de cada endpoint.\newline

El paquete \textbf{persistance} contiene la implementación de un almacén de datos clave/valor muy básico que se utiliza para el almacenamiento de tareas planificadas y las características definidas por el usuario.\newline

Para gestionar la configuración externalizada a través de ficheros YAML, se ha creado el paquete \textbf{configLoader}. Es responsable de la carga de parámetros desde el fichero especificado.\newline

Los paquetes \textbf{dockerMiddleware} y \textbf{dnsdiscovery} contienen la implementación mas significativa para el framework y se detallarán más adelante.

\subsection{Apache JMeter dockerizado}

Existen multitud de imágenes de JMeter disponibles en DockerHub\footnote{https://hub.docker.com/}, todas creadas por la comunidad. Al no existir ninguna imagen oficial, para no dar nada por sentado, se construirá una imagen Docker de Apache JMeter desde el sistema operativo básico para así poder controlar completamente el funcionamiento de la herramienta.\newline

Además se debe ofrecer un modo de configuración a través de variables de entorno para que sea fácilmente ejecutable por línea de comandos para garantizar una integración completa con la automatización.\newline

Las variables de entorno que definirán el modo de operación del contenedor son las siguientes:

\begin{itemize}
  \item \textbf{MODE}
  		\subitem{\textbf{master}}: Nodo principal y coordinador del test
  		\subitem{\textbf{node}}: Nodo esclavo para el test de carga distribuido
  \item \textbf{TEST\_PATH}: Ruta del fichero \*.jmx con el test a ejecutar
  \item \textbf{REMOTES}: Lista separada por comas con las direcciones de los nodos esclavos a usar en el test distribuido
\end{itemize}



\section{Diseño e Implementación}
\label{sec:diseno-implementacion}

Aunque, la planificación de la herramienta se ha realizado con un enfoque bottom-up, el diseño de la misma va a ser expuesto utilizando el enfoque contrario, top-down, es decir, desde lo mas abstracto a lo más específico.

\subsection{API REST y modelo de datos}

El diseño de la interfaz con la que trabajará el usuario que utilice esta aplicación ha sido diseñada meticulosamente utilizando el framework de Go \textit{GoaDesign}, el cual, obliga a definir el diseño de la API con un lenguaje declarativo propio antes de la implementación.\newline

Se valoraron otros frameworks de red para Go, pero ninguno de ellos se enfocaba en primero el diseño y posteriormente la implementación, siendo \textit{Goa} el único que si lo ofrecía. Además, una parte fuerte de este framework es la generación automática de Documentación, ya que genera ficheros OpenAPI compatibles con Swagger\footnote{http://swagger.io}.\newline

La definición del modelo de datos con el que trabaja la API REST está fuertemente ligado a la fase de definición de la API con \textit{Goa}, ya que este framework obliga a definir las estructuras de datos en su propio lenguaje DSL, y es el propio framework el que genera las \textit{structs} de \textit{Go} cuando se genera el código con el comando \textit{goagen} .

\subsubsection{Tarea}

Se definió el concepto \textbf{Tarea} como la definición de las pruebas a ejecutar usando el framework Automation Test Queue.\newline

Una tarea tiene una serie de propiedades tales como:

\begin{itemize}
  \item \textbf{Delay} Segundos a esperar entre el despliegue del servicio de workers y el servicio maestro.
  \item \textbf{Name} Nombre de la tarea a desplegar, es su identificador único en el clúster.
  \item \textbf{WaitCommand} Comando a ejecutar en un contenedor aislado, para asegurar que se han desplegado los contenedores del servicio worker correctamente.
  \item \textbf{Master} Definición del servicio Maestro
  \item \textbf{Worker} Definición del servicio Worker
\end{itemize}

\subsubsection{Definición de Servicio}

Los servicios Maestro y Worker tienen los mismos parámetros de definición, y a nivel de estructura no los diferencia ninguna característica.


En el contexto del framework Automation Test Queue, un servicio es la definición de la imagen Docker a desplegar con una serie de características de despliegue ligadas al propio framework.

\begin{itemize}
  \item \textbf{Alias} Identificador del servicio
  \item \textbf{Args} Argumentos que se ejecutarán en cada contenedor del servicio
  \item \textbf{Environment} Lista de variables de entorno que se configurarán en cada contenedor del servicio. Existe una variable reservada para uso interno del framework, \textbf{WORKER\_CSV\_VIPS}, cuya funcionalidad se detallará en los próximos capítulos.
  \item \textbf{FileID} Identificador de un fichero subido previamente a través del endpoint \textit{/databind/upload} que se montará en el servicio para que su contenido esté disponible en los contenedores del servicio.
  \item \textbf{Image} Nombre de la imagen Docker a utilizar.
  \item \textbf{Replicas} Cantidad de contenedores a desplegar para el servicio, que se corresponde a la cantidad de nodos esclavos a usar en el test de carga.
  \item \textbf{Tty} Parámetro que fuerza la consola interactiva en el servicio.
\end{itemize}

\subsubsection{Endpoint API}

Se ofrecen diferentes acciones descritas a continuación

\begin{itemize}
  \item \textbf{/databind} Operaciones relacionadas con la gestión de archivos
  \subitem \textit{GET /list} Devuelve una lista de los ficheros disponibles en el orquestador
  \subitem \textit{POST /upload} Subir un fichero \*.zip con contenidos para que esté disponible para una futura tarea
  \item \textbf{/monitoring} Monitorización del framework
  \subitem \textit{GET /ping} Devuelve un HTTP 200OK si el orquestador está operativo
  \item \textbf{/swarm} Estado del cluster Swarm
  \subitem \textit{GET /} Devuelve los detalles del Cluster Swarm
  \item \textbf{/task} Operaciones con las tareas a lanzar con el framework
      \subitem \textit{PUT /task} Crea una nueva tarea
      \subitem \textit{DELETE /task/\{id\}} Elimina una tarea ya planificada
      \subitem \textit{GET /task/\{id\}} Inspecciona una tarea planificada
\end{itemize}

Durante el diseño de la API, se tuvieron en cuenta todos los tipos de respuesta que podía ofrecer y el contenido de cada mensaje. Se puede consultar en la definición completa de la API en los anexos


%% TO-DO Incluir en anexo la definición de la API

\subsection{Docker Middleware}

El Middleware de Docker fue una de las primeras tareas realizadas durante el desarrollo del framework. Su función es crítica, se encarga de comunicarse con el demonio de Docker a través del SDK proporcionado por Docker para el lenguaje Golang\footnote{https://github.com/docker/go-docker}.\newline

Se comenzó el proyecto\footnote{https://github.com/mtenrero/AutomationTestQueue} con la primera implementación de Docker SDK\footnote{https://godoc.org/github.com/docker/docker/client}. Esta librería pronto quedó deprecada en favor de \textit{go-docker}, librería especificada en el párrafo anterior.\newline

Como la librería aún estaba indicada como \textit{WIP} , (\textit{WORK-IN-PROGRESS}), se decidió crear un nuevo repositorio por si fuese necesario consultar o volver a los ficheros fuente anteriores. Además, en el primer repositorio, se empezó a trabajar sobre el primer enfoque, basado en el auto registro de servicios.\newline \newline

Ofrece una interfaz para las tareas más sencillas ofrecidas por la API de Docker como:

\begin{itemize}
  \item \textbf{Gestión Volúmenes}
  		\subitem{Bindado}
  		\subitem{Eliminación}
  \item \textbf{Gestión Redes Overlay}
  		\subitem{Creación}
  		\subitem{Agregación}
  		\subitem{Eliminación}
  \item \textbf{Gestión Servicios}
  		\subitem{Creación}
  		\subitem{Eliminación}
  \item \textbf{Tareas auxiliares}
  \item \textbf{Mapeo de configuraciones}
\end{itemize}

Además, ofrece multitud de abstracciones a la estructura definida por el paquete oficial de Docker preconfiguradas para los propósitos del framework.

%% ¿AMPLIAR?

Este paquete también contiene la implementación de la orquestación propia del framework Automation Test Queue debido a la alta dependencia de las funciones implementadas en estos paquetes.

\subsection{Fase de orquestación}

El proceso de orquestación está implementado en el paquete dockerMiddleware. El proceso parte de una definición de una tarea que se recibe vía API HTTP. Se comienza con el despliegue de un servicio global auxiliar, creado específicamente para el proyecto.

\subsubsection{Despliegue del descubridor de servicios}

Este servicio contiene otra aplicación \textit{Go} muy básica creada específicamente para este proyecto. El servicio expone otra API REST cuyo principal cometido, es devolver una lista con las direcciones IP encontradas para un determinado nombre de dominio. Para el correcto funcionamiento de este servicio es necesario que el servidor DNS al que se dirige la petición responda con una lista completa de todos los registros \textbf{A} disponibles para el dominio consultado, y ésto, en el contexto de Docker, sólo es posible si el el servicio identificado por el nombre de dominio proporcionado está desplegado con una configuración de red de DNS Round-Robin. \newline

Se utiliza el paquete \textit{net} proporcionado por defecto por el lenguaje para realizar la resolución de nombres DNS. El único endpoint que expone la API es el siguiente:

\begin{itemize}
	\item \textit{GET /api/:hostname} Devuelve las IPs encontradas en el DNS para el \textit{hostname} indicado
\end{itemize}

Las únicas respuestas posibles por esta API son: \newline

\begin{itemize}
	\item \textbf{HTTP 200:OK} Lista con las direcciones encontradas.
	\item \textbf{HTTP 204: No Content} No se han resuelto nombres para el dominio especificado.
\end{itemize}

La aplicación se ha dockerizado y publicado tanto en GitHub\footnote{https://github.com/mtenrero/dnsrr-discovery-api} como en DockerHub\footnote{https://hub.docker.com/r/tenrero/dnsrr-discovery-api/} con una construcción automatizada, de tal manera, que con cada actualización sobre el repositorio, se construye automáticamente una nueva imagen y se publica en DockerHub para que esté disponible de forma automática para toda la comunidad.\newline

Desde un punto de vista arquitectónico, el servicio Docker se despliega de modo global, exponiendo el mismo puerto para que sea accesible en todos los nodos controladores del clúster y asegurarse la accesibilidad desde el framework Automation Test Queue.

El carácter de este servicio es efímero, una vez que ha cumplido su cometido, al finalizar la orquestación de la tarea, el servicio es eliminado.

\subsubsection{Despliegue de Workers}

Una vez que el servicio de descubrimiento de direcciones está correctamente desplegado, se comienza con el despliegue de los contenedores del servicio Worker. El despliegue se realiza con un servicio configurado con una red DNS Round-Robin para que puedan ser descubiertas todas las direcciones de los contenedores, como se ha explicado con anterioridad, y con la cantidad de réplicas definida en la tarea especificada por el usuario.\newline

Para asegurarse de que todos los nodos están disponibles, de manera genérica, mediante una espera activa, por medio de canales de \textit{Go}, se espera hasta que el descubrimiento de servicios devuelve una cantidad de direcciones igual a la cantidad de réplicas especificadas en la tarea por el usuario. Una vez que se ha detectado la cantidad correcta de contenedores, si se ha especificado en la tarea, se realizan comprobaciones de salud sobre todos los contenedores.\newline

El comando de comprobación de salud es ejecutado en un nuevo contenedor que comparte la misma red de los servicios desplegados para garantizar una seguridad mínima en el sistema. Por un lado, se aísla el sistema host, ya que se ejecuta en un contenedor aislado, sin capacidad de conectarse al socket de Docker, por otro, al solo disponer acceso a la red interna de la tarea, dispone de un acceso muy limitado al resto de recursos de red, lo que limita el riesgo en los posibles comandos a ejecutar por parte del usuario.

Una vez que se ha verificado la salud de todos los contenedores, se procede a la orquestación del servicio Master.

Las direcciones descubiertas se guardan para posteriormente inyectárselas al servicio Master.

\subsubsection{Despliegue de Maestros}

El despliegue del servicio maestro es muy similar al despliegue del servicio Worker, pero con unas particularidades. El modo de red es configurado como replicado y VirtualIP,en lugar de DNS Round-Robin, con una única replica a desplegar, de esta manera se permite que el contenedor exponga puertos hacia el exterior en caso de que sea necesario.\newline

%% Describir finalización de servicios.

Una vez que se ha finalizado la orquestación del servicio Master, se elimina el servicio Discovery desplegado al inicio de la orquestación de la tarea.

\subsubsection{Visión general}

En el siguiente diagrama se expone un esquema del proceso general de orquestación de tareas del middleware Automation Test Queue.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{orchestration_steps}
    \caption{Diagrama del proceso de orquestación de tareas}
    \label{fig:orchestration_steps}
\end{figure}

\subsection{Almacenamiento de datos}

Se ha utilizado por simplicidad el paquete \textit{buntdb}\footnote{https://github.com/tidwall/buntdb} para el almacenamiento de datos del framework. Se basa en un esquema clave/valor, siendo compatible con otras soluciones de almacenamiento de datos distribuidos similares, como etcd\footnote{https://coreos.com/etcd/}.\newline

Todas las operaciones relacionadas con el almacenamiento de datos se encuentran en el paquete \textbf{Persistence}. En el lanzamiento del framework, durante las primeras instrucciones, se realiza una llamada al método \textit{InitPersistence()}, el cual devuelve un struct del tipo \textit{*Persistence} que contiene la instancia inicializada de \textit{BuntDB} y ésta es inyectada sobre los controladores que hacen uso de ella, como es el caso del controlador \textbf{Task}.\newline

Debido a la naturaleza del lenguaje \textit{Go} y la implementación del paquete \textit{BuntDB}, se ha creado un wrapper para el almacenamiento y la obtención de datos para gestionar las llamadas de una manera más simple y comprensible.\newline

Se encuentra en el fichero \textit{wrapper.go} y ofrece los métodos

\begin{itemize}
\item \textbf{store(key, value string) error} Almacena una nueva entrada.
\item \textbf{read(key string) (string, error)} Obtiene el valor de una clave dada.
\item \textbf{delete(key string) error} Elimina una entrada del almacenamiento de datos.
\item \textbf{iterateStringString(index string) (*map[string]string, error)} Devuelve una colección de entradas dado un patrón de indexado existente.
\end{itemize}

Para conseguir un funcionamiento similar a objetos con múltiples propiedades, \textit{BuntDB} ofrece una solución abstracta, la definición de índices mediante wildcards.

Por ello, se ha configurado un índice para permitir obtener múltiples propiedades indicando un identificador de tarea, definiendo el patrón de almacenamiento de datos \textit{task:*:} \newline

El uso de un almacenamiento de datos es necesario para almacenar el estado actual de la tarea y los identificadores de los directorios bindados a cada servicio. Como restricción se ha implementado un control de duplicidades, para asegurarse de que no se desplieguen dos tareas con el mismo identificador.

\subsection{Imágenes Docker}

Para todas las herramientas y el middleware desarrollado se proporcionan los ficheros \textit{Dockerfile} para que sea posible construir las imágenes de las mismas y poder así desplegarlas de forma nativa en Docker.\newline

No obstante, se han enlazado los repositorios de GitHub con Docker Hub mediante la configuración de webhooks, para que con cada commit o Pull-Request se construyan las imágenes de forma automática y sean publicadas a Docker Hub y esté disponible para la comunidad la versión más reciente de la utilidad en el registro público de manera completamente automatizada.\newline

Por otra parte, para el middleware, también se proporciona un fichero \textit{docker-compose} (Apéndice B.1 y B.2) para que pueda ser desplegado en una infraestructura existente de Docker Swarm como un nuevo Stack.

\subsection{Service Discovery}

Para el descubrimiento de servicios, como se ha avanzado en algún párrafo anterior, se ha desarrollado una mínima aplicación \textit{Go} que expone una API REST sencilla. Esta aplicación aprovecha el enrutado interno de los contenedores de los servicios docker que estén configurados con el algoritmo DNS RR.\newline

Una vez que se ha realizado la consulta al servidor de nombres, se parsea la información y se devuelve en formato \textit{CSV}, es decir, en valores separados por comas, ya que es el formato que JMeter requiere para especificar los nodos remotos y es muy fácilmente iterable por cualquier script para así poder emplear los datos con futuras herramientas.\newline

El descubrimiento de servicios también es empleado en el algoritmo interno de orquestación del middleware, ya que éste para asegurarse de que los contenedores han sido correctamente inicializados, realiza peticiones asíncronamente con una espera activa hasta que se recibe la cantidad deseada de contenedores a ser desplegados. Y una vez que se puede acceder a dichos contenedores, si el usuario lo ha especificado, se procede a ejecutar el comando que se asegura de la disponibilidad de los contenedores desplegados a través de peticiones tcp o udp.

\section{Integración Continua}
\label{sec:integracion-continua}

Inicialmente, el proyecto, al estar alojado en el servicio de control de versiones de GitHub, en un repositorio de código abierto, se ofrecían diversas opciones SAAS de Integración Continua, entre ellas TravisCI o CircleCI.


Al comenzar el desarrollo, las características que ofrecía TravisCI encajaban perfectamente con los requisitos de Automation Test Queue:

\begin{itemize}
  \item Pipeline-as-code
  \item Enfocado a Contenedores
  \item Integración con GitHub
\end{itemize}

Se utilizó durante la mitad de desarrollo del proyecto y ofreció muy buenos resultados. Pero llegó un momento en el que era necesario que el host donde se ejecutaban los tests cada vez que realizaba un commit formase parte de un cluster de Docker Swarm, y esto no se podía conseguir con la solución que ofrecía TravisCI.\newline

La limitación de TravisCI era muy importante ya que rompía la línea de Integración Continua, por lo que no quedó mas remedio que optar por una solución mas configurable, Jenkins.

No se optó desde un principio por Jenkins debido a la necesidad de disponer de una máquina con acceso a la red las 24 horas al día y la configuración del mismo.\newline

El despliegue de las máquinas destinadas a la línea de integración continua se realizó sobre máquinas EC2 de Amazon Web Services, todas ellas bajo el mismo grupo de disponibilidad y con un servicio de IP elástica, permitiendo de esta manera que la dirección de acceso a Jenkins fuese estática. Además se incluyó un registro A en el DNS de mi dominio particular para poder acceder con una dirección\footnote{http://atq.mtenrero.com:8080} mas fácil de recordar aún.

Una parte vital para el correcto funcionamiento de la línea de integración continua fue configurar webhooks en GitHub para apuntar a la instancia de Jenkins además de habilitar la integración propia de Jenkins. Esto permitió que con cada evento en el repositorio, como \textit{commits} o \textit{pull-requests} se enviase una notificación a Jenkins para así poder ejecutar el job especificado y lanzar los tests unitarios sobre todas las ramas del proyecto para asegurar la regresión y un buen funcionamiento del mismo.\newline


Hace apenas dos años, Cloudbees, la organización que es oficialmente responsable del desarrollo de Jenkins, lanzó una característica que llamaron \textit{Jenkins Declarative Pipelines} \footnote{https://jenkins.io/blog/2017/02/03/declarative-pipeline-ga/}. Estos pipelines, permiten escribir, de una forma desriptiva, las tareas y procesos a realizar cada vez que el job es ejecutado.

Esto ha permitido describir el workflow de ejecución de manera totalmente agnóstica y compatible con cualquier Jenkins desplegado, siempre que tenga Docker instalado, ya que hace uso del denominado Docker-in-Docker, que permite utilizar el demonio de Docker del host dentro de un contenedor ya existente.


\section{Pruebas}
\label{sec:pruebas}

Desde el comienzo del proyecto, la parte de las pruebas sobre la aplicación ha sido muy importante debido a la aplicación de las metodologías ágiles y TDD, ademas del uso de la librería Goa con la que se ha realizado la interfaz REST del Middleware.\newline

Se ha organizado el proyecto siguiendo el estándar de \textit{Go}, es decir, dentro de cada paquete se han alojado sus tests unitarios. Los tests se diferencian del código en el nombre de fichero, ya que por convenio, los ficheros test en Go deben finalizar con la palabra \textit{\_test.go}.\newline

Es común encontrar diferentes ficheros de test en cada paquete ya que se ha seguido un estilo de distribución de ficheros por funcionalidad para así ofrecer un mayor facilidad para comprender el código.\newline

Tomando como ejemplo el paquete \textit{\textbf{persistence}} se han realizado un total de seis ficheros de test:

\begin{itemize}
  \item \textbf{databind\_test.go} Contiene los tests relacionados con las operaciones sobre la entidad \textit{databind}, encargada de las operaciones con los ficheros que proporciona el usuario para sus tests.
  \item \textbf{filesystem\_test.go} Contiene los tests de almacenamiento de la base de datos clave/valor sobre el sistema de ficheros del sistema.
  \item \textbf{persistence\_test.go} Comprueba la escritura y la lectura básica utilizando la implementación de almacenamiento.
  \item \textbf{task\_test.go} Contiene los tests sobre todas las posibles operaciones relacionadas con la entidad Task del Middleware.
  \item \textbf{timestamper\_test.go} Comprueba el funcionamiento correcto del generador de UUIDs basado en Timestamp.
  \item \textbf{wrapper\_test.go} Tests relacionados sobre el wrapper que encapsula y simplifica el uso de la librería BuntDB.
\end{itemize}

La mayoría de los ficheros anteriormente indicados, se corresponden conceptualmente con un fichero con el mismo nombre sin la terminación \_test. Esos ficheros contienen la implementación de la funcionalidad descrita.\newline

De esta manera, se ofrece a un futuro desarrollador, además de unas pruebas que aseguren una regresión, un ejemplo de cómo usar las implementaciones ofrecidas, ya que se han realizado pruebas para prácticamente la totalidad de la implementación realizada salvo una mínima excepción.

\subsection{Tecnologías utilizadas}

Para realizar un seguimiento sobre la cobertura de tests se ha utilizado la plataforma Coveralls.io\footnote{https://coveralls.io/github/mtenrero/ATQ-Director}. Permite visualizar gráficamente la evolución de la cobertura entre los diferentes commits realizados al control de versiones.\newline

Para el correcto uso de esta plataforma ha sido necesario apoyarse en la librería \textit{Goveralls}\footnote{https://github.com/mattn/goveralls}. Esta librería envía los resultados de las pruebas a la plataforma Coveralls.io. Durante el cambio a Jenkins como sistema de Integración Continua, Goveralls no ofrecía soporte completo para este sistema de CI y fue necesario introducir algunos cambios en el repositorio\footnote{https://github.com/mtenrero/goveralls} mediante un Pull Request\footnote{https://github.com/mattn/goveralls/pull/116} para mejorar la compatibilidad con Jenkins.\newline

Se ha utilizado el paquete de testing nativo de \textit{Go} y el paquete \textit{Testify}\footnote{github.com/stretchr/testify/assert} que proporciona una serie de funciones y utilidades para simplificar el proceso de testeo, como comparaciones, expected conditions, etc...

\subsection{Cobertura de tests}

En un primer lugar se realizaron tests básicos sobre los casos de uso necesarios y posteriormente, estos tests se fueron ampliando para disponer de una regresión completa en la mayoría de paquetes.

\subsubsection{Docker Middleware}

El paquete dockerMiddleware dispone de una cobertura de test del 42,02\%, y esto es debido a que las pruebas a realizar sobre este paquete en realidad pertenecen mas a pruebas de integración ya que requieren comunicarse con el demonio de Docker del sistema y requieren una serie de pre comprobaciones. No obstante, se han implementado la mayor parte de pruebas posibles, sobre todo para las operaciones más básicas como crear y eliminar redes, contenedores y volúmenes. 

\subsubsection{Cargador de configuraciones}

El paquete configLoader dispone de una cobertura de tests del 100\%, ya que es el encargado de la lectura y propagación de configuraciones desde un fichero YAML.

\subsubsection{Persistence}

El paquete de gestión de la persistencia de datos dispone de una cobertura de test del 87,56\%, cubriendo casi la totalidad de la funcionalidad. Quedando fuera de la regresión ciertos casos muy difíciles de conseguir al estar relacionados propiamente con la librería, pero como se tienen en cuenta en los test el valor esperado por la llamada a dichas funciones, se puede afirmar casi con total seguridad que se podría detectar prácticamente cualquier fallo.