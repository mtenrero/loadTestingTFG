\cleardoublepage
\chapter{Descripción informática}


\section{Requisitos}
\label{sec:requisitos}

Tras un análisis del proyecto a desarrollar se llegó a la conclusión de que había que seguir un enfoque top-down, o de arriba hacia abajo. Era necesario que durante toda la etapa del desarrollo, se tuviese muy claro a dónde se pretendía llegar. Siendo los objetivos principales los siguientes:

\begin{itemize}
  \item Capacidad de desplegar los servicios Docker necesarios para una tarea específica
  \item Networking automático y transparente
  \item Interfaz sencilla para el usuario
  \item Integración completa con Apache JMeter
\end{itemize}

Así se mostró en el tablero Kanban del proyecto, proporcionado por GitHub para el repositorio concreto\footnote{https://github.com/mtenrero/ATQ-Director}.


\section{Arquitectura y Análisis}
\label{sec:arquitectura-analisis}


Golang fue el lenguaje de programación elegido debido a su alta simplicidad en cuanto a concurrencia se refiere, por permitir ser compilado y generar binarios para los mayores Sistemas Operativos y debido a la existencia de un paquete distribuido por Docker para comunicarse directamente con la API que ofrece el demonio de Docker.

La mayor complejidad presente en este proyecto ha sido obtener las direcciones virtuales de cada contenedor desplegado sobre el cluster Swarm, ya que son necesarias para poder comunicarse con las instancias directamente. Docker, por defecto, cuando crea una red entre servicios, puede invocar directamente al nombre del servicio para acceder a él, pero en el caso de que se despligue el servicio en modo replicado y éste haya sido configurado para tener más de una replica, por defecto, Docker actúa como un balanceador de carga y sólo responde con una única dirección IP correspondiente a un único contenedor. Esto ha sido un duro handicap, ya que se requería conocer todas las direcciones de los contenedores desplegados.\newline


Durante el análisis de cómo afrontar esta problemática, surgieron dos enfoques totalmente diferentes


\subsection{Primer enfoque: Starter en cada imagen a desplegar}

Este enfoque surge tomando como referencia la mayoría de herramientas para service discovery usadas en Docker y en Kubernetes como Consul.io\footnote{https://www.consul.io/}. 

Cada contenedor a desplegar, tiene una imagen modificada, la cual, antes de ejecutar el entrypoint predefinido en la imagen, ejecuta una pequeña aplicación, la cual, obtiene la IP virtual del contenedor y se la comunica al agente controlador, el cual lleva un listado de todas los contenedores descubiertos. 

%% INSERTAR IMAGEN CON LA ARQUITECTURA

Este enfoque, implicaba modificar todas las posibles imágenes a usar con la aplicación, con lo que se reducía drásticamente la facilidad de uso con otras aplicaciones al requerir que en caso de no existir la imagen modificada deseada a desplegar con Automation Test Queue, obligaría al usuario a modificarla por él mismo.

\subsection{Segundo enfoque: Single Daemon y DNS Round Robin}

Con una configuración de red entre servicios Docker configurada para operar con el algoritmo DNS Round Robin, se consigue disponer de la lista de todas las direcciones IP Virtuales de los contenedores de un mismo servicio.\newline

Disponiendo de esta información, dejaría de ser necesario un descubrimiento de servicios como el expuesto en el anterior punto, dotando a la aplicación de la sencillez de uso deseada debido a que el usuario ya no tendría que modificar la imagen a utilizar.

%% INSERTAR IMAGEN CON LA ARQUITECTURA

\subsection{Estructura del proyecto}

El proyecto está íntegramente codificado en \textit{Go}. Sigue una estructura típica de este lenguaje, siempre teniendo en cuenta que se mencionan rutas relativas a la ubicación del repositorio (/github.com/mtenrero/ATQ-Director) bajo el \$GOPATH definido en el sistema.\newline

\dirtree{%
.1 \textbf{app} Código autogenerado por \textit{Goa}.
.1 \textbf{client} Implementación de un cliente en Go, autogenerado.
.1 \textbf{configLoader} Implementación de la configuración externalizada.
.1 \textbf{dnsdiscovery} Implementación discovery DNS.
.1 \textbf{dockerMiddleware} Middleware Docker.
.1 \textbf{http}.
.2 \textbf{design} Diseño API REST con el DSL de \textit{Goa}.
.1 \textbf{persistance} Implementación persistencia datos.
.1 \textbf{swagger} Especificación OpenAPI autogenerada.
.1 \textbf{tool} Helper autogenerado para trabajar con la API.
.1 \textbf{types} Mapeado tipos Goa/Middleware.
.1 \textit{monitoring.go} Controlador Monitoring.
.1 \textit{swarm.go} Controlador Swarm.
.1 \textit{task.go} Controlador Task.
.1 \textit{databind.go} Controlador Databind.
}

En el esquema anteriormente presentado, se puede apreciar que la implementación ha sido fragmentada en grandes paquetes para cumplir el estándar de \textit{Go} y de los principios \textit{KISS} y \textit{SOLID}.\newline

En la raiz del directorio del proyecto se pueden apreciar unos ficheros con extensión \textit{.go}, contienen los controladores para cada endpoint definido con el DSL de \textit{Goa} en el paquete \textit{http/design}. Se genera la estructura básica en la primera ejecución de \textit{goagen} y a partir de ahí se integra la implementación propia de cada endpoint.\newline

El paquete \textbf{persistance} contiene la implementación de un almacén de datos clave/valor muy básico que se utiliza para el almacenamiento de tareas planificadas y las características definidas por el usuario.\newline

Para gestionar la configuración externalizada a través de ficheros YAML, se ha creado el paquete \textbf{configLoader}. Es responsable de la carga de parámetros desde el fichero especificado.\newline

Los paquetes \textbf{dockerMiddleware} y \textbf{dnsdiscovery} contienen la implementación mas significativa para el framework y se detallarán más adelante.

\section{Diseño e Implementación}
\label{sec:diseno-implementacion}

Aunque, la planificación de la herramienta se ha realizado con un enfoque bottom-up, el diseño de la misma va a ser expuesto utilizando el enfoque contrario, top-down, es decir, desde lo mas abstracto a lo más específico. 

\subsection{API REST y modelo de datos}

El diseño de la interfaz con la que trabajará el usuario que utilice esta aplicación ha sido diseñada meticulosamente utilizando el framework de Go \textit{GoaDesign}, el cual, obliga a definir el diseño de la API con un lenguaje declarativo propio antes de la implementación.\newline

Se valoraron otros frameworks de red para Go, pero ninguno de ellos se enfocaba en primero el diseño y posteriormente la implementación, siendo \textit{Goa} el único que si lo ofrecía. Además, una parte fuerte de este framework es la generación automática de Documentación, ya que genera ficheros OpenAPI compatibles con Swagger\footnote{http://swagger.io}.\newline

La definición del modelo de datos con el que trabaja la API REST está fuertemente ligado a la fase de definición de la API con \textit{Goa}, ya que este framework obliga a definir las estructuras de datos en su propio lenguaje DSL, y es el propio framework el que genera las \textit{structs} de \textit{Go} cuando se genera el código.

\subsubsection{Tarea}

Se definió el concepto \textbf{Tarea} como la definición de las pruebas a ejecutar usando el framework Automation Test Queue.\newline 

Una tarea tiene una serie de propiedades tales como:

\begin{itemize}
  \item \textbf{Delay} Segundos a esperar entre el despliegue del servicio de workers y el servicio maestro.
  \item \textbf{Name} Nombre de la tarea a desplegar, es su identificador único en el clúster.
  \item \textbf{WaitCommand} Comando a ejecutar en un contenedor aislado, para asegurar que se han desplegado los contenedores del servicio worker correctamente.
  \item \textbf{Master} Definición del servicio Maestro
  \item \textbf{Worker} Definición del servicio Worker
\end{itemize}

\subsubsection{Definición de Servicio}

Los servicios Maestro y Worker tienen los mismos parámetros de definición, y a nivel de estructura no los diferencia ninguna característica.


En el contexto del framework Automation Test Queue, un servicio es la definición de la imagen Docker a desplegar con una serie de características de despliegues ligadas al propio framework.

\begin{itemize}
  \item \textbf{Alias} Identificador del servicio
  \item \textbf{Args} Argumentos que se ejecutarán en cada contenedor del servicio
  \item \textbf{Environment} Lista de variables de entorno que se configurarán en cada contenedor del servicio. Existen una variable reservadas para uso interno del framework, \textbf{WORKER\_CSV\_VIPS}, cuya funcionalidad se detallará en los próximos capítulos.
  \item \textbf{FileID} Identificador de un fichero subido previamente a través del endpoint \textit{/databind/upload} que se montará en el servicio para que su contenido esté disponible en los contenedores del servicio.
  \item \textbf{Image} Nombre de la imagen Docker a utilizar.
  \item \textbf{Replicas} Cantidad de contenedores a desplegar para el servicio
  \item \textbf{Tty} Parámetro que fuerta la consola interactiva en el servicio.
\end{itemize}

\subsubsection{Endpoint API}

Se ofrecen diferentes acciones descritas a continuación

\begin{itemize}
  \item \textbf{/databind} Operaciones relacionadas con la gestión de archivos
  \subitem \textit{GET /list} Devuelve una lista de los ficheros disponibles en el orquestador
  \subitem \textit{POST /upload} Subir un fichero \*.zip con contenidos para que esté disponible para una futura tarea
  \item \textbf{/monitoring} Monitorización del framework
  \subitem \textit{GET /ping} Devuelve un HTTP 200OK si el orquestador está operativo
  \item \textbf{/swarm} Estado del cluster Swarm
  \subitem \textit{GET /} Devuelve los detalles del Cluster Swarm
  \item \textbf{/task} Operaciones con las tareas a lanzar con el framework
      \subitem \textit{PUT /task} Crea una nueva tarea
      \subitem \textit{DELETE /task/\{id\}} Elimina una tarea ya planificada
      \subitem \textit{GET /task/\{id\}} Inspecciona una tarea planificada
\end{itemize}

Durante el diseño de la API, se tuvo en cuenta todos los tipos de respuesta que puede ofrecer y el contenido de cada mensaje. Se puede consultar en la definición completa de la API en los anexos


%% Incluir en anexo la definición de la API

\subsection{Docker Middleware}

El Middleware de Docker fue uno de las primeras tareas realizadas durante el desarrollo del framework. Su función es crítica, se encarga de comunicarse con el demonio de Docker a través del SDK proporcionado por Docker para el lenguaje Golang\footnote{https://github.com/docker/go-docker}.\newline

Ofrece una interfaz para las tareas más sencillas ofrecidas por la API de Docker como:

\begin{itemize}
  \item \textbf{Gestión Volúmenes}
  		\subitem{Bindado}
  		\subitem{Eliminación}
  \item \textbf{Gestión Redes Overlay}
  		\subitem{Creación}
  		\subitem{Agregación}
  		\subitem{Eliminación}
  \item \textbf{Gestión Servicios}
  		\subitem{Creación}
  		\subitem{Eliminación} 
  \item \textbf{Tareas auxiliares}
  \item \textbf{Mapeo de configuraciones}
\end{itemize}

Además, ofrece multitud de abstracciones a la estructura definida por el paquete oficial de Docker preconfiguradas para los propósitos del framework.

%% ¿AMPLIAR?

Este paquete también contiene la implementación de la orquestación propia del framework Automation Test Queue debido a la alta dependencia de estos paquetes.

\subsection{Fase de orquestación}

El proceso de orquestación está implementado en el paquete dockerMiddleware. El proceso parte de una definición de una tarea que se recibe vía API HTTP. Se comienza con el despliegue de un servicio global auxiliar, creado específicamente para el proyecto. 

\subsubsection{Despliegue del descubridor de servicios}

Este servicio contiene otra aplicación \textit{Go} muy básica creada específicamente para este proyecto. El servicio expone otra API REST cuyo principal cometido, es devolver una lista con las direcciones IP encontradas para un determinado nombre de dominio. Para el correcto funcionamiento de este servicio es necesario que el servidor DNS al que se dirige la petición responda con una lista completa de todos los registros A disponibles para el dominio consultado, y ésto, en el contexto de Docker, sólo es posible si el el servicio identificado por el nombre de dominio proporcionado está desplegado con una configuración de red de DNS Round-Robin. \newline

Se utiliza el paquete \textit{net} proporcionado por defecto por el lenguaje para realizar la resolución de nombres DNS. El único endpoint que expone la API es el siguiente: 

\begin{itemize}
	\item \textit{GET /api/:hostname} Devuelve las IPs encontradas en el DNS para el \textit{hostname} indicado
\end{itemize}

Las únicas respuestas posibles por esta API son: \newline

\begin{itemize}
	\item \textbf{HTTP 200:OK} Lista con las direcciones encontradas.
	\item \textbf{HTTP 204: No Content} No se han resuelto nombres para el dominio especificado.
\end{itemize}

La aplicación se ha dockerizado y publicado tanto en GitHub\footnote{https://github.com/mtenrero/dnsrr-discovery-api} como en DockerHub\footnote{https://hub.docker.com/r/tenrero/dnsrr-discovery-api/} con una construcción automatizada, de tal manera, que con cada actualización sobre el repositorio, se construye automáticamente una nueva imagen y se publica en DockerHub para que esté disponible de forma automática para toda la comunidad.\newline

Desde un punto de vista arquitectónico, el servicio Docker se despliega de modo global, exponiendo el mismo puerto para que sea accesible en todos los nodos controladores del clúster y asegurarse la accesibilidad desde el framework Automation Test Queue.

El carácter de este servicio es efímero, una vez que ha cumplido su cometido, al finalizar la orquestación de la tarea, el servicio es eliminado. 

\subsubsection{Despliegue de Workers}

Una vez que el servicio de descubrimiento de direcciones está correctamente desplegado, se comienza con el despliegue de los contenedores del servicio Worker. El despliegue se realiza con un servicio configurado con una red DNS Round-Robin para que puedan ser descubiertas todas las direcciones de los contenedores, como se ha explicado con anterioridad, y con la cantidad de réplicas definida en la tarea especificada por el usuario.\newline

Para asegurarse de que todos los nodos están disponibles, de manera genérica, mediante una espera activa, por medio de canales de \textit{Go}, se espera hasta que el descubrimiento de servicios devuelve una cantidad de direcciones igual a la cantidad de réplicas especificadas en la tarea por el usuario. Una vez que se ha detectado la cantidad correcta de contenedores, si se ha especificado en la tarea, se realizan comprobaciones de salud sobre todos los contenedores.\newline

El comando de comprobación de salud es ejecutado en un nuevo contenedor que comparte la misma red de los servicios desplegados para garantizar una seguridad mínima en el sistema. Por un lado, se aísla el sistema host, ya que se ejecuta en un contenedor aislado, sin capacidad de conectarse al socket de Docker, por otro, al solo disponer acceso a la red interna de la tarea, dispone de un acceso muy limitado al resto de recursos de red, lo que limita el riesgo en los posibles comandos a ejecutar por parte del usuario. 

Una vez que se ha verificado la salud de todos los contenedores, se procede a la orquestación del servicio Master.

Las direcciones descubiertas se guardan para posteriormente inyectárselas al servicio Master.

\subsubsection{Despliegue de Maestros}

El despliegue del servicio maestro es muy similar al despliegue del servicio Worker, pero con unas particularidades. El modo de red es configurado como replicado y VirtualIP,en lugar de DNS Round-Robin, con una única replica a desplegar, de esta manera se permite que el contenedor exponga puertos hacia el exterior en caso de que sea necesario.\newline

%% Describir finalización de servicios.

Una vez que se ha finalizado la orquestación del servicio Master, se eliminar el servicio Discovery desplegado al inicio de la orquestación de la tarea.

\subsection{Imágenes Docker}
\subsection{Service Discovery}

\section{Integración Continua}
\label{sec:integracion-continua}

Inicialmente, el proyecto, al estar alojado en el servicio de control de versiones de GitHub, en un repositorio de código abierto, se ofrecían diversas opciones SAAS de Integración Continua, entre ellas TravisCI o CircleCI.


Al comenzar el desarrollo, las características que ofrecía TravisCI encajaban perfectamente con los requisitos de Automation Test Queue:

\begin{itemize}
  \item Pipeline-as-code
  \item Enfocado a Contenedores
  \item Integración con GitHub
\end{itemize}

Se utilizó durante la mitad de desarrollo del proyecto y ofreció muy buenos resultados. Pero llegó un momento en el que era necesario que el host donde se ejecutaban los tests cada vez que realizaba un commit formase parte de un cluster de Docker Swarm, y esto no se podía conseguir con la solución que ofrecía TravisCI.\newline

La limitación de TravisCI era muy importante ya que rompía la línea de Integración Continua, por lo que no quedó mas remedio que optar por una solución mas configurable, Jenkins.

No se optó desde un principio por Jenkins debido a la necesidad de disponer de una máquina con acceso a la red las 24 horas al día y la configuración del mismo.\newline

El despliegue de las máquinas destinadas a la línea de integración continua se realizó sobre máquinas EC2 de Amazon Web Services, todas ellas bajo el mismo grupo de disponibilidad y con un servicio de IP elástica, permitiendo de esta manera que la dirección de acceso a Jenkins fuese estática. Además se incluyó un registro A en el DNS de mi dominio particular para poder acceder con una dirección\footnote{http://atq.mtenrero.com:8080} mas fácil de recordar aún.

Una parte vital para el correcto funcionamiento de la línea de integración continua fue configurar webhooks en GitHub para apuntar a la instancia de Jenkins además de habilitar la integración propia de Jenkins. Esto permitió que con cada evento en el repositorio, como \textit{commits} o \textit{pull-requests} se enviase una notificación a Jenkins para así poder ejecutar el job especificado y lanzar los tests unitarios sobre todas las ramas del proyecto para asegurar la regresión y un buen funcionamiento del mismo.\newline


Hace apenas dos años, Cloudbees, la organización que es oficialmente responsable del desarrollo de Jenkins, lanzó una característica que llamaron \textit{Jenkins Declarative Pipelines} \footnote{https://jenkins.io/blog/2017/02/03/declarative-pipeline-ga/}. Estos pipelines, permiten escribir, de una forma desriptiva, las tareas y procesos a realizar cada vez que el job es ejecutado.

Esto ha permitido describir el workflow de ejecución de manera totalmente agnóstica y compatible con cualquier Jenkins desplegado, siempre que tenga Docker instalado, ya que hace uso del denominado Docker-in-Docker, que permite utilizar el demonio de Docker del host dentro de un contenedor ya existente.


\section{Pruebas}
\label{sec:pruebas}

Hablar de Cobertura de Test, como se ha organizado por paquetes, cómo está integrado con el Pipeline de IC de Jenkins, los triggers que tiene configurado...
\newline

Explicar cómo se diseñaron primero los tests básicos, luego la implementación, y después se amplió la cobertura de tests
